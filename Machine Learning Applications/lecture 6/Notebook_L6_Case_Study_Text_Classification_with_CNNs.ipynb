{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f64bBXyhpxTR"
   },
   "source": [
    "# Case study 2: Sentiment Analysis with CNN Models\n",
    "\n",
    "\n",
    "Sentiment analysis is one of the most widely performed analysis on text data. Sentiment analysis has been through tremendous improvements from the days of classic methods to recent times where in the state of the art models utilize deep learning to improve the performance.\n",
    "\n",
    "Convolutional Neural Networks or CNNs have brought deep learning research into mainstream discussions, especially on image classification and image and video recognition.\n",
    "\n",
    "In this case study, we will perform sentiment analysis using a CNN in a dataset of 50,000 movie reviews (IMDB dataset that you can found on Kaggle at https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iayiP3GZrjjk",
    "outputId": "95b899a3-1c3d-49fb-f2fe-bb4d31197513"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\python310\\lib\\site-packages (4.62.3)\n",
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: colorama in c:\\users\\apala\\appdata\\roaming\\python\\python310\\site-packages (from tqdm) (0.4.4)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\apala\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install tqdm\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Gbnh0egkUzRX"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv1D, MaxPooling1D, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# fix random seed for reproductibility\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7pQ5WR1VYqm",
    "outputId": "09f78573-f26c-460a-d281-6581c3f6cb86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('IMDB Dataset.csv')\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "XMP7r5cUV6-B",
    "outputId": "ad141833-0761-41eb-b0b5-f911975b14ec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a peek at the data\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing & arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. As for the shots with Jake: just ignore them.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['review'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "XpwueLn6V-qF"
   },
   "outputs": [],
   "source": [
    "# Split the dataset between train set and test set\n",
    "reviews = dataset['review'].values\n",
    "sentiments = dataset['sentiment'].values\n",
    "\n",
    "train_reviews = reviews[:35000]\n",
    "train_sentiments = sentiments[:35000]\n",
    "\n",
    "test_reviews = reviews[35000:]\n",
    "test_sentiments = sentiments[35000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CETaiGM2tO_d"
   },
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "tkFyu9u3tUOi"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # Beautiful Soup is a Python library for pulling data out of HTML and XML files.\n",
    "import numpy as np\n",
    "import re # To handle regular expression\n",
    "import tqdm # Instantly make your loops show a smart progress meter - just wrap any iterable with tqdm(iterable), and you’re done!\n",
    "import unicodedata\n",
    "\n",
    "# Function to remove html tags \n",
    "#Beautiful Soup is a Python library for pulling data out of HTML and XML files. \n",
    "def strip_html_tags(text):\n",
    "  soup = BeautifulSoup(text, \"html.parser\")\n",
    "  [s.extract() for s in soup(['iframe', 'script'])]\n",
    "  stripped_text = soup.get_text()\n",
    "  stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "  return stripped_text\n",
    "\n",
    "# Function to remove accents\n",
    "def remove_accented_chars(text):\n",
    "  text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "  return text\n",
    "\n",
    "# Function to remove html tags\n",
    "def pre_process_corpus(docs):\n",
    "  norm_docs = []\n",
    "  for doc in tqdm.tqdm(docs):\n",
    "    doc = strip_html_tags(doc)\n",
    "    doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "    doc = doc.lower()\n",
    "    doc = remove_accented_chars(doc)\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A) #Replace anything except 0..9, a..z and A..Z\n",
    "    doc = re.sub(' +', ' ', doc) #remove +\n",
    "    doc = doc.strip()  #remove whitespaces\n",
    "    norm_docs.append(doc)\n",
    "  \n",
    "  return norm_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Cecile\n"
     ]
    }
   ],
   "source": [
    "print (unicodedata.normalize('NFKD', u'\\u2460'))\n",
    "print(unicodedata.normalize('NFKD', 'Cécile').encode('ascii', 'ignore').decode('utf-8', 'ignore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EgAqgSnHtap5",
    "outputId": "6fa30098-d4c6-4dfe-89bb-569e2ad517c8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35000/35000 [00:21<00:00, 1595.87it/s]\n",
      "100%|██████████| 15000/15000 [00:09<00:00, 1566.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 s\n",
      "Wall time: 31.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "norm_train_reviews = pre_process_corpus(train_reviews)\n",
    "norm_test_reviews = pre_process_corpus(test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COgKPRmLpxTu"
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "To prepare text data for our deep learning model, we transform each review into a sequence.\n",
    "Every word in the review is mapped to an integer index and thus the sentence turns into a sequence of numbers.\n",
    "\n",
    "To perform this transformation, keras provides the ```Tokenizer```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "dff8sG63cw03"
   },
   "outputs": [],
   "source": [
    "t = Tokenizer(oov_token='<UNK>')\n",
    "# fit the tokenizer on the documents of the train set\n",
    "t.fit_on_texts(norm_train_reviews)\n",
    "# will be useful for padding (see later)\n",
    "t.word_index['<PAD>'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11, 110, 849, 2873]]\n"
     ]
    }
   ],
   "source": [
    "# Example of what the instance tokenizer does to text data\n",
    "print(t.texts_to_sequences(['I love deep learning']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0xd_eGZ1vQRR",
    "outputId": "8fe0ce5d-4ec3-4644-88a1-7b8192be5bb6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('dawgis', 176790), ('<PAD>', 0), 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Outputs are: the word with the maximum token value, the word with the minimum token value\n",
    "# and the token value when the word is unknown (i.e. not in the train set vocabulary)\n",
    "max([(k, v) for k, v in t.word_index.items()], key = lambda x:x[1]), min([(k, v) for k, v in t.word_index.items()], key = lambda x:x[1]), t.word_index['<UNK>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# token value if word=<PAD>\n",
    "t.word_index['<PAD>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# token value if word is unknown\n",
    "t.word_index['<UNK>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "4yv_m8T5c2xg"
   },
   "outputs": [],
   "source": [
    "train_sequences = t.texts_to_sequences(norm_train_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ifZCCxtydEnc"
   },
   "outputs": [],
   "source": [
    "test_sequences = t.texts_to_sequences(norm_test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ldkDHyjZgaFV",
    "outputId": "be8fbb65-b9a7-4572-d312-2099537608c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size=176791\n",
      "Number of Documents=35000\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size={}\".format(len(t.word_index)))\n",
    "print(\"Number of Documents={}\".format(t.document_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "CQjiXA7Ntw13",
    "outputId": "517a0efc-f6a8-427d-8d96-c83c0748dae1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "train_lens = [len(s) for s in train_sequences]\n",
    "test_lens = [len(s) for s in test_sequences]\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(12, 6))\n",
    "h1 = ax[0].hist(train_lens)\n",
    "h2 = ax[1].hist(test_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfZwP6C8pxT8"
   },
   "source": [
    "### Sequence Normalization\n",
    "\n",
    "Not all reviews are of same length. To handle this difference in length of reviews, we define a maximum length.\n",
    "For reviews which are smaller than this length, we pad them with zeros which longer ones are truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "BtuGJ0wXjQnC"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wAnv99kzWA5k",
    "outputId": "c6619d01-6083-4470-9b26-e0f73e864c17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 1000), (15000, 1000))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad dataset to a maximum review length in words\n",
    "X_train = sequence.pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test = sequence.pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11, 110, 849, 2873, 1]]\n",
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0   11  110  849 2873    1]]\n"
     ]
    }
   ],
   "source": [
    "# Example of what pad_sequences function does\n",
    "text_to_sequence_example=t.texts_to_sequences(['I love deep learning azertyyyyy'])\n",
    "print(text_to_sequence_example)\n",
    "print(sequence.pad_sequences(text_to_sequence_example, maxlen=MAX_SEQUENCE_LENGTH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9X_4ticSpxUC"
   },
   "source": [
    "### Encoding Labels\n",
    "\n",
    "The dataset contains labels of the form positive/negative. The following step encodes the labels using ```sklearn's``` ```LabelEncoder```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "rRMaWb1ldqyl"
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "num_classes=2 # positive -> 1, negative -> 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "tJjbtyDjfsd1"
   },
   "outputs": [],
   "source": [
    "y_train = le.fit_transform(train_sentiments)\n",
    "y_test = le.transform(test_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "iaqFz7ZpdoLC"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(t.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176791"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCjRYBh2pxUM"
   },
   "source": [
    "## Prepare the Model\n",
    "\n",
    "Since textual data is a sequence of words, we utilize ```1D``` convolutions to scan through the sentences.\n",
    "The model first transforms each word into lower dimensional embedding/vector space followed by 1d convolutions and then passing the data through dense layers before the final layer for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "LR3mdd8kjgW1"
   },
   "outputs": [],
   "source": [
    "EMBED_SIZE = 300\n",
    "EPOCHS=2\n",
    "BATCH_SIZE=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AXhAERVeXhmZ",
    "outputId": "09482215-10fe-4c56-9217-58d6ca8203e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1000, 300)         53037300  \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 1000, 128)         153728    \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 500, 128)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 500, 64)           32832     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 250, 64)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 250, 32)           8224      \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 125, 32)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4000)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               1024256   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 54,256,597\n",
      "Trainable params: 54,256,597\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(VOCAB_SIZE, EMBED_SIZE, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(Conv1D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "szNl8QiQpxUa"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0uc0jXszf5ob",
    "outputId": "bb5b4088-332b-424a-e759-b1c4162a542d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "247/247 [==============================] - 514s 2s/step - loss: 0.4141 - accuracy: 0.7692 - val_loss: 0.2476 - val_accuracy: 0.8986\n",
      "Epoch 2/2\n",
      "247/247 [==============================] - 494s 2s/step - loss: 0.1336 - accuracy: 0.9514 - val_loss: 0.2573 - val_accuracy: 0.9023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21f3d6d9930>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, \n",
    "          validation_split=0.1,\n",
    "          epochs=EPOCHS, \n",
    "          batch_size=BATCH_SIZE, \n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuKczZqYpxUk"
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Zik9CWQgNlK",
    "outputId": "72e63b1d-13d4-45b0-9d91-14b3b09c7091"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 13s 28ms/step - loss: 0.2731 - accuracy: 0.8960\n",
      "Accuracy: 89.60%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "B904TLKNiA1B"
   },
   "outputs": [],
   "source": [
    "predictions = (model.predict(X_test) > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HVjbqjaopxU-",
    "outputId": "96addda4-5642-46a1-8cb0-c76e1caa238e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'positive']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = ['positive' if item == 1 else 'negative' for item in predictions]\n",
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "mINpo7mDpxVC",
    "outputId": "4b1dc581-846f-4030-ca42-8ec79e19319d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.86      0.89      7490\n",
      "    positive       0.87      0.93      0.90      7510\n",
      "\n",
      "    accuracy                           0.90     15000\n",
      "   macro avg       0.90      0.90      0.90     15000\n",
      "weighted avg       0.90      0.90      0.90     15000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>6477</td>\n",
       "      <td>1013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>511</td>\n",
       "      <td>6999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          negative  positive\n",
       "negative      6477      1013\n",
       "positive       511      6999"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "labels = ['negative', 'positive']\n",
    "print(classification_report(test_sentiments, predictions))\n",
    "pd.DataFrame(confusion_matrix(test_sentiments, predictions), index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of reviews wrongly classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrects = np.nonzero(predictions != test_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([    9,    32,    38, ..., 14958, 14975, 14993], dtype=int64),)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2146"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrects[0][200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As someone who loves baseball history, especially the early 20th century in which Cobb was a main figure, along with a ton of colorful characters, I was looking forward to seeing this baseball film. Well, it wasn't a baseball film, which was disappointing. No, it was just a sportswriter's account of being with Cobb in the ballplayer's later years while the two collaborated on a book. Even at that, this could have been a more appealing movie than they made it.<br /><br />Granted Cobb was anything but a nice guy, an extremely talented player but brutal in that he would do anything to beat you....and he was viscous, intimidating and had a lot of demons to fight. He was so hated his own teammates tried to hinder his chances of winning a batting title one year. Nonetheless, this an over-the-top portrayal of the man. It makes him into something almost cartoon-like. <br /><br />Watching and listening to an old man rant, rave and profane for two hours is entertainment? No, it isn't. Some day, I'd love to see a real biopic of Cobb showing him in his ballplaying days and if they want to portray him as an evil guy, so be it, but the way they did it here with just a bitter, blasphemous old man making an ass of himself in front of a reporter is not fun to watch. || negative\n"
     ]
    }
   ],
   "source": [
    "print(test_reviews[14958],'||',test_sentiments[14958])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I contend that whoever is ultimately responsible for creating/approving the trailer for this movie has completely blundered. NO ONE I know wanted to see this movie based on the previews, and EVERYONE who actually saw it (that I know) absolutely loved it... The advertising campaign is disgrace/disaster/blunder.<br /><br />Opened at #4 behind...<br /><br />#1-Rush Hour, which I have not seen, average IMDb score of 7.4.<br /><br />#2-The Bourn Ultimatum, which I have seen, awesome movie but 3rd week out, average IMDb score of 8.7 (deserving I would say).<br /><br />#3-The Simpsons Movie, which I have seen, okay movie but 4th week out, average IMDb score of 8.1 (a bit high in my opinion).<br /><br />#4-Stardust, average IMDb score of 8.4 (lower then Bourn, but that's been our for 3 weeks).<br /><br />Whether it was poor scheduling or poor advertising I think that the powers that be behind this movie screwed up big time! This should have been advertised as an amazing movie that happens to be a fantasy/fairytale and not advertised as just another fairytale\n",
      " Too bad :( Anyway- Now that I have very pointlessly ranted on-and-on... Awesome movie, go see it! || positive\n"
     ]
    }
   ],
   "source": [
    "print(test_reviews[2146],'||',test_sentiments[2146])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I saw the film at the Belgrade Film Festival last week, and I'm still working off the trauma. Essentially my view seems to match a number of others - the first half hour was fresh, sharp, deep, entertaining and promising. Well acted too. Natural. My problem, however, is not simply with the fact that the final hour and a half of the film have nothing to do with the likable beginning, nor the fact that I spent most of this time convulsing in agony at sharp, grating industrial sounds and squinting at drunken, toothless, bread-chewing hags. It's rather with the fact that THEY NEVER WARNED ME!!! The festival brochure synopsis described only the (utterly intriguing-sounding) first half hour - a whore, piano tuner and meat seller chat in a bar, pretending to be an advertising agent, genetic engineer, and petty government administration official, respectively - making no mention whatsoever of the never-ending gum-smacking to come. Serves me right for not reading the reviews, you might say - but to my defense, a number of reviews I looked at post-fact um didn't at all stress the immensity and utter unbearableness of the greater part of the film.<br /><br />The first hint should have been the introductory words by the director (a bashful, tousle-haired Russian youth) who stepped in front of the crammed auditorium (the film seems to be doing incredibly well critically, and tickets were sold out well in advance of the screening, though most of the audience seemed as unaware as I was of the pain to come, judging by the plethora of unearthly moans and groans that utterly permeated the theatre during the last half hour, and many exasperated comments on exit) to say the following: 'Well, I... um, thank you very much for coming to see this film, and I just wanted to say... well, it's a very long film... it took me four years to make it, and... it's.. I suggest that you see it and immediately try to forget about it. It is very long. Thank you for coming.' This is what he said. Alarm bells should have been ringing. 'What's he talking about?' I thought in happy confusion. 'This is gonna be fun!' Of course, by the time his strangely apologetic comments started making sense to me, it was far too late to get out. All I could do is writhe in increasing agony until the lights came on again. And in the end I can't say I feel in any way improved by the experience. Yes, I absolutely loved the first half hour. It was intelligent, new, and had a lot to say. And yes, Russia is probably in a bad state. Yes, every society has many hidden faces. Yes, toothless life in barren wastelands is probably unimaginably hard. Yes yes yes. I get all of this. Really I do. But I see no earthly reason why art and meaning should be so agonisingly drawn out, and so painful to bear. If you want to see a film land somewhere between the extremes of glitzy Hollywood plastic fantastic and hours of muddy vodka swigging, try the Korean-Chinese Bin Jip (3-Iron). It's artsy and surprising, but also to-the-point and fun. || negative\n"
     ]
    }
   ],
   "source": [
    "print(test_reviews[135],'||',test_sentiments[135])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A charming boy and his mother move to a middle of nowhere town, cats and death soon follow them. That about sums it up.<br /><br />I'll admit that I am a little freaked out by cats after seeing this movie. But in all seriousness in spite of the numerous things that are wrong with this film, and believe me there is plenty of that to go around, it is overall a very enjoyable viewing experience.<br /><br />The characters are more like caricatures here with only their basis instincts to rely on. Fear, greed, pride lust or anger seems to be all that motivate these people. Although it can be argued that that seeming failing, in actuality, serves the telling of the story. The supernatural premise and the fact that it is a Stephen King screenplay(not that I have anything specific against Mr. King) are quite nicely supported by some interesting FX work, makeup and quite suitable music. The absolute gem of this film is without a doubt Alice Krige who plays Mary Brady, the otherworldly mother.<br /><br />King manages to take a simple story of outsider, or people who are a little different(okay - a lot in this case), trying to fit in and twists it into a campy over the top little horror gem that has to be in the collection of any horror fan. || negative\n"
     ]
    }
   ],
   "source": [
    "print(test_reviews[9],'||',test_sentiments[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Text Classification - Deep Learning CNN Models.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
