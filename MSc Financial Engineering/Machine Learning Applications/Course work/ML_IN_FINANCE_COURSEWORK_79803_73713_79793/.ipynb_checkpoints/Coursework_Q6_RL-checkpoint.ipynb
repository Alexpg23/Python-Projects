{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by setting the transition probabilities, rewards and possible actions for the Markov process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probabilities = [\n",
    "    [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0]],\n",
    "    [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n",
    "    [None, [0.8, 0.1, 0.1], None]]\n",
    "\n",
    "rewards = [\n",
    "    [[+50, 0, 0], [0, 0, 0], [0, 0, 0]], \n",
    "    [[0, 0, 0], [0, 0, 0], [0, 0, -250]],\n",
    "    [[0, 0, 0], [+200, 0, 0], [0, 0, 0]]]\n",
    "\n",
    "possible_actions = [[0, 1, 2], [0, 2], [1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare a matrix for the Q-values with the dimensions of the action-state space: 3 by 3 in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_values = np.full((3, 3), -np.inf) # -np.inf for impossible actions \n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q_values[state, actions] = 0.0 # for all possible actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Bellman equation we calculate the Q-values and discount them at the set rate of 0.95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.95 # the discount factor\n",
    "for iteration in range(50): \n",
    "    Q_prev = Q_values.copy() \n",
    "    for s in range(3):\n",
    "        for a in possible_actions[s]: \n",
    "            Q_values[s, a] = np.sum([transition_probabilities[s][a][sp]\n",
    "                                    * (rewards[s][a][sp] + gamma * np.max(Q_prev[sp])) \n",
    "                                for sp in range(3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the Q-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[109.49259367, 104.01778116,  84.33432283],\n",
       "       [  5.60048953,         -inf,   5.89544459],\n",
       "       [        -inf, 269.36381834,         -inf]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the optimal actions will be a-0 in state 0, a-2 in state 1 and the only possible a-1 in state 3.\n",
    "\n",
    "Interestingly, the difference between the Q-values of actions 0 & 1 in state 0 and 0 & 2 in state 1 are not that different, meaning it may be worth to try them out. But according to calculations, the actions with the highest 'scores' are the best choice.\n",
    "\n",
    "We can also see how high is the Q-value of a-1 in state 3, as it is the only possible action and all the positive rewards from the process are concentrated in this one score."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "32b7c5b9f98a1a97705439c220b701a8c9e57eb8efef980bcb082a82a900a3ed"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pypr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
