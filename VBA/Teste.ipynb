{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "314dca8c",
   "metadata": {},
   "source": [
    "## Final Assignment\n",
    "\n",
    "### Team Members:\n",
    "\n",
    "#### Marcel Santos de Carvalho, id 79803\n",
    "#### Alex Palacios, id 73713\n",
    "\n",
    "## Question 1\n",
    "### 1.1 Data obtention\n",
    "\n",
    "In this question, we will be analyzing different characteristics from 13 different stocks to better understand their behavior. The selected assets were:\n",
    "\n",
    "- SPY: SPDR S&P 500 ETF Trust\n",
    "- F: Ford\n",
    "- AMZN: Amazon\n",
    "- AAPL: Apple\n",
    "- WFC: Wells Fargo\n",
    "- MSFT: Microsoft\n",
    "- NVDA: Nvidia\n",
    "- AMD: Advanced Micro Devices\n",
    "- BAC: Bank of America\n",
    "- GE: General Electric\n",
    "- XOM: Exxon Mobil\n",
    "- JPM: JPMorgan Chase\n",
    "- GS: Goldman Sachs\n",
    "\n",
    "First, we downloaded the series of prices for each stock from Januayr 1st 2001 to August 30th 2021 from Yahoo Finance. Then we created a VBA procedure to generate the daily returns of each stock as follows:\n",
    "\n",
    "$$r_{t+1}=ln(\\frac{p_{t+1}}{p_{t}})$$\n",
    "\n",
    "Afterwards, we created another procedure that computes the tracking error fo each stock against a prespecified benchmark. In this case, the benchmark is the SPY ETF itself. This series of tracking error will be used later on the analysis. At the moment we defined the tracking error as:\n",
    "\n",
    "$$Active Share=r_{i}-r_{\\beta}$$\n",
    "\n",
    "Next, we created a procedure that creates a table with the main characteristics of each stock such as:\n",
    "\n",
    "- Daily returns\n",
    "    - $\\sum_{n=1}^{N}\\frac{r_{n}}{N}$\n",
    "- Annualized returns\n",
    "    - $Daily returns*252$\n",
    "- Daily Standard Deviation\n",
    "    - $\\sqrt{Daily Variance}$\n",
    "- Annualized Standard Deviation\n",
    "    - $Daily Standard Deviation * \\sqrt{252}$\n",
    "- Daily Variance\n",
    "    - $\\sum_{n=1}^{N}\\frac{r_{n}-\\bar{r}}{N}$\n",
    "- Annualized Variance\n",
    "    - $ Daily Variance * 252$\n",
    "    \n",
    "Below, you can see the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a602d8f",
   "metadata": {},
   "source": [
    "![text](Images/Q1/Statistics.png)\n",
    "*Fig.1 - Statistics based on returns from 01/01/2001 to 30/08/2021*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f549608",
   "metadata": {},
   "source": [
    "There are several stocks jump to the sight mainly for they high annualized return and annualized risk. Among them, AMD and NVDA have higher risk while AMZN and GE have thi higher Expected return.\n",
    "\n",
    "Having computed the returns, we created a new procedure helping us to print the Variance-Covariance Matrix and the Correlation Matrix. The rationale behind the computation of the Variance-Covariance Matrix is the following. We define the returns vector as:\n",
    "\n",
    "$$ R = \\begin{pmatrix}1\\\\\n",
    "r_{1} \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    "r_{n} \n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Then the Variance-Covariance Matrix is computed by applying the following formula:\n",
    "\n",
    "$$Var-Cov = E[R_{i}-E[R_{i}]]*E[R_{j}-E[R_{j}]]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76e6f66",
   "metadata": {},
   "source": [
    "![text](Images/Q1/VarCovar.png)\n",
    "*Fig.2 - Variance Covariance matrix based on returns from 01/01/2001 to 30/08/2021*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c312d6",
   "metadata": {},
   "source": [
    "In the case of the correlation matrix, it is computed simply by dividing the covariance found in the previous step by the product of the standard deviation of both assets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a2acc1",
   "metadata": {},
   "source": [
    "![text](Images/Q1/Corr.png)\n",
    "*Fig.3 - Correlation matrix based on returns from 01/01/2001 to 30/08/2021*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5c075d",
   "metadata": {},
   "source": [
    "This tables are highly important to measure the sensitivity of a portfolio to a systematic and non systematic risk exposures, but they are also useful when performing a single-asset analysis as they allow us to compute the $\\beta$ of a stock. We will comment on that measure later on this work.\n",
    "\n",
    "Following this idea of analyzing different risk measures of single assets, we built a procedure to compute the Value at Risk and the Expected Shortfall of each stock. The Value at Riks stablishes the maximum loss a portfolio can suffer at a given confidence level. For example, if the confidence level is set at $1-\\alpha$%, then if several paths were simulated, assuming some density probability function, the stock is expected to loss at least VaR% $alpha$% of the time.\n",
    "\n",
    "In our case, the VaR is dialy given that our prices are daily. This can also be thought as for any given day, each stock could loose up to VaR with a confidence level of $1-\\alpha$%. To estimate the VarR we used the Historical VaR method. This method is based on sorting the observed returns and finding the value from which the lowest $\\alpha$ returns start. That value is chosen as the empirical estimation of the VaR. Below you can see a table with our estimations at 95% and 98% confidence levels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d90ffe",
   "metadata": {},
   "source": [
    "![text](Images/Q1/Var.png)\n",
    "*Fig.4 - Historical VaR at 95% and 98% confidence levels based on returns from 01/01/2001 to 30/08/2021*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751eadde",
   "metadata": {},
   "source": [
    "It important to notice how the differences between stocks increase substantially wen increasing the confidence level from 95% to 98%. In the latter is easy to observe that NVDA and AMD are the more risky stocks. \n",
    "\n",
    "Next, the Expected Shortfall is simply the expected value of observations below the VaR. This can be understood as, given that we pass the VaR, how much do we expect to loose. It can be seen mathematicalli as:\n",
    "\n",
    "$$E[r|r\\le VaR]$$\n",
    "\n",
    "This can be estimated on historical observations similarly as the VaR. What we did was to compute the average of the observations below the VaR. As we did with the VaR, we computed the Expected Shortfall for both confidence levels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557034d5",
   "metadata": {},
   "source": [
    "![text](Images/Q1/Exp_Shortfall.png)\n",
    "*Fig.5 - Historical Expected Shortfall at 95% and 98% confidence levels based on returns from 01/01/2001 to 30/08/2021*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95175b1f",
   "metadata": {},
   "source": [
    "As we can see, the results suggested by the VaR tables are confirmed, being NVDA and AMD the stocks with the highest Expected Shortfall. This measure is more insightful for investors as it allows to quantify in a better way the risk exposure. \n",
    "\n",
    "Finally, we computed table showing the following ratios:\n",
    "\n",
    "- Sharpe ratio\n",
    "    - $Sharpe = \\frac{E[r_{i}]-r_{f}}{\\sigma_{M}}$\n",
    "- Treynor ratio\n",
    "    - $Treynor = \\frac{E[r_{i}]-r_{f}}{\\beta_{i}}$\n",
    "- Information ratio\n",
    "    - $Information = \\frac{Active return}{Active risk}$\n",
    "        - Where Active return is the average of the tracking error we computed in the first steps and\n",
    "        - Active risk is the standard deviation of the Active return\n",
    "        \n",
    "You can see the results obtained in the table below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2645986b",
   "metadata": {},
   "source": [
    "![text](Images/Q1/Ratios.png)\n",
    "*Fig.6 - Performance ratios based on returns from 01/01/2001 to 30/08/2021*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2989238",
   "metadata": {},
   "source": [
    "It is important to explain that the expected return of our stocks are not simply the average of the past returns returns. We derived the expected return by using the Capital Asset Pricing Model, which stablishes the following:\n",
    "\n",
    "$$E[r_{i}=r_{f}+\\beta_{i}*E[R_{M}-r_{f}]]$$ \n",
    "\n",
    "Here, having computed the Variance-Covariance matrix comes in handy as the $\\beta$ is defined as:\n",
    "\n",
    "$$\\beta=\\frac{Cov(r_{i},R_{M})}{var(R_{M})}$$\n",
    "\n",
    "Higher betas reflect a higher exposition to the systematic risk. In the case of our chosen stocks, they have higher than one $\\beta$s which is not surprising given that these are among the stocks with highest market capitalization in the market, which gives them a large weight in the market index.\n",
    "\n",
    "For the risk-free rate, $r_{f}$, we downloaded the US 1 Yield interest rate time series with yearly observations from 2994 until 2001. We then computed the market premium comparing hte observed value of r_{f} and the SPY at each point in time. Then, both the market premium and the risk-free rate are the average of the observed values. \n",
    "\n",
    "Nonetheless, defining the stock's expected return in such way had a major consequence. All the Treynor ratios were the same for all the stocks. This makes sense since an underlying assumption of the CAPM is that it is not possible to generate $\\alpha$ or abnormal returns. If this holds, the Treynor ratio is exactly equal to the expected market risk premium for all the stocks. If we had used as the historical average as the expected returns, probably we wouldn't get the same number for all stocks, showing that in reality the CAPM hardly holds. \n",
    "\n",
    "In the case of the Sharpe ratio, it as a risk-adjusted return measure. Meaning that it allows us to know which stock compensates the most per unit of risk. In our case, none of our stocks have a Sharpe ratio higher than the market. This entails the real benefits from diversification.\n",
    "\n",
    "Finally, the information ratio shows how much benefit is a portfolio getting from the active management profile of their investments. In our case, all stocks have information ratios near zero. This is somewhat expected given the high betas from the stocks and the lower-than-the-market Sharpe ratios, which suggest higher level of risks overall.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dd6dfc",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "### Vasicek Calibration\n",
    "\n",
    "In this question, we will be pricing options based on a Monte Carlo simulation of the Stock price evolution based on a Geometric Brownian Motion Process. But, rather that assuming a constant level for the interest rate, we will allow the short term interest rate to fluctuate along time based on the the Vasicek Model. The Vasicek model defines the change in interest rates as follows:\n",
    "\n",
    "$$dr(t)=k[\\theta-r(t)]dt+\\sigma dB^{\\mathbb{Q}}(t)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\theta$ is the mean reversion level\n",
    "- $k$ is the mean reversion speed\n",
    "- $\\sigma$ is the volatility of the short-term interest rate\n",
    "- $B^{\\mathbb{Q}}$ is a Brownian Motion process under the risk-neutral probability space $\\mathbb{Q}$\n",
    "\n",
    "To calibrate the model we downloaded the monthly observations of the FED Funds rate from January 2001 to November 2001 from the St. Louis Fed site. We transformed the rate into its continous rate equivalent. Then we computed the actual change in the interest rate and we compared them with the drift component of the above equation. This difference, a.k.a., the error, should follow a normal distribution. This can be seen in the following equation:\n",
    "\n",
    "$$dr(t)-k[\\theta-r(t)]dt=\\sigma dB^{\\mathbb{Q}}(t) \\sim {N(0,\\sigma)}$$\n",
    "\n",
    "Knowing this, we computed the error from the real change in the interest rate and what should be observed given some random parameters. Then we computed the Probability Density Function from a Normal distrubtion with mean 0 and volatility $\\sigma$ evaluated at the error. Finally, we took the natural logarithim of that number and compute the sum. This is done to calibrate the model and find the optimal values for $\\theta$, $k$ and $\\sigma$. The optimal values would be the ones that maximizes the following equation:\n",
    "\n",
    "$$Objective_Fucntion=\\sum_{i=1}^{N}ln(PDF(x_{i}))$$\n",
    "\n",
    "We followed this process fixing some of the parameters in the next manner:\n",
    "\n",
    "1. In the first calibration, we allowed the three parameters to be determined by the calibration\n",
    "2. In the second calibration, we fixed the mean reverting level and solved for the other two parameters\n",
    "3. In the third calibration, we fixed the volatility and solved for the remaining parameters\n",
    "\n",
    "We took these different calibrations as our base model for the path of the interest ratesl. In the table below you can see the parameters that we obtained. As well as the parameters chosen to run out simulation for the interest rates future paths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfcfaf7",
   "metadata": {},
   "source": [
    "![text](Images/Q2a/Vasicek-Parameters.png)\n",
    "*Fig.7 - Vasicek calibrated parameters based on historical observations from 2001 to 2021*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b5be60",
   "metadata": {},
   "source": [
    "As we can see, on the first simulation, we obtained a value for $\\theta$ too close to zero. This seems reasonable on a first look given the level of interest rates from the last years. Although, we must recognize that the last years could be considered as an atypical period. Below, you can see a graph with the historical level of interest rates. Here, it might actually seems as if the interest rates were being pulled towards 0.00%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8b1143",
   "metadata": {},
   "source": [
    "![text](Images/Q2a/Market_data-Historical_rates.png)\n",
    "*Fig.8 - FED funds interest rate from 2001 to 2021*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e54056",
   "metadata": {},
   "source": [
    "Nonetheless, we choose to run different calibrations by fixing one parameter as we found a very low level for the mean reversion level, $\\theta$, which was not consistent with the yield curve observed in the market. To illustrate this point, we downloaded the yield curve from the St. Louis Fed site as of December 8th 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee7bbd7",
   "metadata": {},
   "source": [
    "![text](Images/Q2a/Market_data-30Y_Curve.png)\n",
    "*Fig.9 - US Yield Curve as of 12/08/2021*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78da6152",
   "metadata": {},
   "source": [
    "From this graph is quite clear that a 0.08% for a long-term mean reversion level is not very close to reality. This is more evident if we zoom in the graph in the short term range of 0 to 3 years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7a80d5",
   "metadata": {},
   "source": [
    "![text](Images/Q2a/Market_data-3Y_Curve.png)\n",
    "*Fig.10 - US Yield Curve short-term as of 12/08/2021*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4f10cd",
   "metadata": {},
   "source": [
    "From both graphs, it is clear that the expecations for the short-term interest rates are strongly upward baised. We decided to propose an long-term mean reversion level more close to the actual market expectations. To do so, we downloaded the blue dots graph in which we can see the the expectations from the Federal Open Market Committee. This is a widely used source to get a sense about the future level of the FED target rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cca6b8f",
   "metadata": {},
   "source": [
    "![text](Images/Q2a/Market_data-Blue_Dots.png)\n",
    "*Fig.10 - Blue Dots graph as of 12/08/2021*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c28e22",
   "metadata": {},
   "source": [
    "In the case of the level we fixed for $\\sigma$, we simply chose the historical volatility. Quite interestingly, the mean reversion speed did not changed much even when we fixed some of the parameters. Now, the we found the parameters, we simulated a 1000 different paths with 252 steps per year for 3 years for each different set of parameters that we obtained. We sorterd the paths based on the last observation for the short-term interest rate and we chose the middle path. We created a sub that does both process at once and the resulting simulations looked as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87934e55",
   "metadata": {},
   "source": [
    "![text](Images/Q2a/Analytical-Simulated_Yield_Curves.png)\n",
    "*Fig.11 - Simulated path for short-term interest rate*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08953d85",
   "metadata": {},
   "source": [
    "Based on the rate paths, the Vasicek framework makes it possible to extrapolate an implicit yield curve accounting for the convexity. In out case, it would look as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4971d6a6",
   "metadata": {},
   "source": [
    "![text](Images/Q2a/Analytical-Simulated_Yield_Paths.png)\n",
    "*Fig.12 - Implicit Yield Curve*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ef7700",
   "metadata": {},
   "source": [
    "As we can see, the third curve, which is the one in we fixed the volatility goes inverted at some point eventhough the mean reverting level is positive. This is somewhat contradictory to the yield curves observed in the market. This can be a consequence of the small value of the mean revertion speed. The other two paths have a significant lower volatility, which compensates the low revertion speed. Finally, it is important to notice that the mean reverting level doesn't seems to have had a signigicant impact between the first and the second path. Again, this can be also a direct consequence of the low reversion speed in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c3c4e8",
   "metadata": {},
   "source": [
    "### Stock's price simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534f0556",
   "metadata": {},
   "source": [
    "Now that we have three different possible rate paths we will simulate the evolution of 6 different stocks under each of the three different scenarios for the interest rates. This means that we will end up having 18 different simulations. The simulations will be done under the Black & Scholes framework, in which the stock is assumed to evolve following a Geometric Brownian Motion as follows:\n",
    "\n",
    "$$\\frac{dS_{t}}{S_{t}}=r_{f} dt+\\sigma dW^{\\mathbb{Q}}$$\n",
    "\n",
    "Under the risk-neutral probability space, $\\mathbb{Q}$, the drift of the process is set to be $r_{f}$ Nonetheless, we will introduce a small change to the previous equation by allowing r to vary. Now, the equation would look as follows:\n",
    "\n",
    "$$\\frac{dS_{t}}{S_{t}}=r_{f}(t) dt+\\sigma dW^{\\mathbb{Q}}$$\n",
    "\n",
    "By applying the Itto's Lemma, it is possible to find the solution for this process, which we will be using to run our simulations. The solution for the above process is:\n",
    "\n",
    "$$S_{t+1}=S_{t}\\mathrm{e}^{\\Big[(r_{f}(t) -\\frac{\\sigma^2}{2})dt+ \\sigma dW_{t} \\Big]}$$\n",
    "\n",
    "We will run 300 simulations with a time frame of three years with 252 steps per year. We created a procedure that runs the 18 simulations taking the following parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d621b6f",
   "metadata": {},
   "source": [
    "![text](Images/Q2b/GBM-Parameters.png)\n",
    "*Fig.13 - Inputs for simulation*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2212e2",
   "metadata": {},
   "source": [
    "Once we had the simulation for the stock prices, we applied an inbuilt procedure to compute the payoff of an option initiating today depending of it beng a Put or a Call with a strike set to be 90% of the Stock's today price. With the computed averages we took the average and then discount the value using the following discount factor:\n",
    "\n",
    "$$Discount_Factor=e^{\\sum_{i=1}^{N}r_{i}dt}$$\n",
    "\n",
    "Where $r_{i}$ are de discount rates found in each of the paths. This means that we have three different discount factors, one for each simulation done. This should approximate the real value of the option. To test the accuracy or our results, we computed the real Black & Scholes price using the following equations:\n",
    "\n",
    "$$ C(S_t,t) = N(d_1)S_t-N(d_2)Ke^{-rt}$$\n",
    "\n",
    "$$ P(S_t,t) = -N(-d_1)S_t+N(-d_2)Ke^{-rt}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$  d_1=\\frac{ln\\frac{S_t}{K}+(r+\\frac{\\sigma^2}{2})t}{\\sigma\\sqrt{t}}$$\n",
    "\n",
    "$$ d_2=d_1-\\sigma\\sqrt{t}$$\n",
    "\n",
    "In this case the r used in the Black and Scholes formula was chose to equal exactly the discount factor described above. For this reason, there are also three different prices for the Black and Scholes case. The results are shown in the table below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd308f11",
   "metadata": {},
   "source": [
    "![text](Images/Q2b/GBM-Results.png)\n",
    "*Fig.14 - Option Pricing Black and Scholes versus Monte Carlo*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148502a1",
   "metadata": {},
   "source": [
    "One interesting fact is that the prices from the Monte Carlo simulation obtained in the third scenario, where the curve got inverted, did not varied much from the other two simulations where the yield curves were more flat. This supports the idea of the Black and Scholes framework, in which we price the option in a risk-neutral probability space. This means that the real evolution of the stock does not impact the price today of the option. We proved this by actually allowing the interest rate to move and to follow three different paths. Some prices are still far from the real value but this can be a consequence of the number of simulations done.\n",
    "\n",
    "After doing this, we developed a procedure to obtained the price for a Put and a Call when one parameter changes and the others remain constant. This is helpful to assess the exposure that the investor has to the change in one of the parameters. This can be useful to hedge those risks that the investors is not prepared to face. We present the results below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180862fb",
   "metadata": {},
   "source": [
    "##### Sensitivity to interest rate\n",
    "\n",
    "First, we obtained the prices for the options for different levels of the interest rates ranging from -10.0% to 10.0%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada96667",
   "metadata": {},
   "source": [
    "![text](Images/Q2b/GBM-Call_sens_to_interest.png)\n",
    "*Fig.15 - Call option value as the interest rate changes*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481cd0f8",
   "metadata": {},
   "source": [
    "![text](Images/Q2b/GBM-Put_sens_to_interest.png)\n",
    "*Fig.16 - Put option value as the interest rate changes*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe0bfac",
   "metadata": {},
   "source": [
    "As we can see, in the graph above the Call price increases as the rho increases, while for the Put it is the other way around. This is makes sense if you think in the binomial pricing model for an option. You could borrow the strike, buy the stock and then sell the stock to pay the loan. As interest rate rates increases the cost of borrowing goes up and, therefore, the call premium. In the case of the Put is all the way around. This sensitivity is represented by Rho."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b878eb",
   "metadata": {},
   "source": [
    "##### Sensitivity to volatility\n",
    "\n",
    "\n",
    "Next, we obtained the prices for the options for different volatilities levels ranging from 0.0% to 100.0%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d3a652",
   "metadata": {},
   "source": [
    "![text](Images/Q2b/GBM-Call_sens_to_vol.png)\n",
    "*Fig.17 - Call option value as the volatility changes*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4059f2",
   "metadata": {},
   "source": [
    "![text](Images/Q2b/GBM-Put_sens_to_vol.png)\n",
    "*Fig.18 - Put option value as the volatility changes*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab18e687",
   "metadata": {},
   "source": [
    "In the case of the volatility, the option prices increases as the volatility increases in both cases. This result is more easy to see if we think that the optionality of taking a decision in the future on an asset that do not moves at all is somewhat worthless. As the uncertainity increases, the optionality gains value. This sensitivity is represented by Vega."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f985d852",
   "metadata": {},
   "source": [
    "##### Sensitivity to time\n",
    "\n",
    "Next, we obtained the prices for the options for times ranging from 0.0 to 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c351ed2",
   "metadata": {},
   "source": [
    "![text](Images/Q2b/GBM-Call_sens_to_time.png)\n",
    "*Fig.19 - Call option value as time varies*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576feaa8",
   "metadata": {},
   "source": [
    "![text](Images/Q2b/GBM-Put_sens_to_time.png)\n",
    "*Fig.20 - Put option value as the interest rate changes*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68163f5",
   "metadata": {},
   "source": [
    "In the case of time, the same reasoning applies. It is worht more for any investor to have the opportunity to take a decision, either to sell or to buy, one week in the future than having to do so tomorrow. This sensitivity is called Theta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b59af7",
   "metadata": {},
   "source": [
    "##### Sensitivity to dividend yield\n",
    "\n",
    "Finally, we obtained the prices for the options for dividend yields ranging from 0.0% to 10.0%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36fe2a6",
   "metadata": {},
   "source": [
    "![text](Images/Q2b/GBM-Call_sens_to_dividend_yield.png)\n",
    "*Fig.21 - Call option value as dividend yield varies*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea630c36",
   "metadata": {},
   "source": [
    "![text](Images/Q2b/GBM-Put_sens_to_dividend_yield.png)\n",
    "*Fig.22 - Put option value as dividend yield varies*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dd5cdf",
   "metadata": {},
   "source": [
    "Finally, in the case of the dividend yields. You can think of it as an inverse cost of opportunity. In the binomial tree approximation described above, the dividend yield partially offsets the cost of borrowing, therefore decreasing the call premium. As in the case of Rho, the sensitivity is to the opposite direction in the case of the Put. This sensitivity has no name.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c01bcb3",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe53a12e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
